{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T06:26:59.278628Z","iopub.execute_input":"2024-05-29T06:26:59.279000Z","iopub.status.idle":"2024-05-29T06:27:00.124010Z","shell.execute_reply.started":"2024-05-29T06:26:59.278962Z","shell.execute_reply":"2024-05-29T06:27:00.123092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:28:36.567169Z","iopub.execute_input":"2024-05-29T06:28:36.567533Z","iopub.status.idle":"2024-05-29T06:28:38.589714Z","shell.execute_reply.started":"2024-05-29T06:28:36.567506Z","shell.execute_reply":"2024-05-29T06:28:38.588793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\n","metadata":{}},{"cell_type":"code","source":"# Create a copy of the DataFrame\ncleaned_train_df = df.drop(['id', 'FloodProbability'], axis=1).copy()\n\n# Define the multiplier for identifying outliers based on standard deviation\nmultiplier = 3\n\n# Calculate the upper outlier threshold for each column\nupper_thresholds = cleaned_train_df.mean() + multiplier * cleaned_train_df.std()\n\n# Identify rows containing outliers in each column\noutlier_rows = (cleaned_train_df > upper_thresholds).any(axis=1)\n\n# Drop outlier rows from the DataFrame\ncleaned_train_df = cleaned_train_df[~outlier_rows]\ncleaned_target_df = df.loc[~outlier_rows,'FloodProbability']\n\n# Print the shape of the cleaned DataFrame\nprint(\"Original DataFrame shape:\", df.shape)\nprint(\"Cleaned DataFrame shape:\", cleaned_train_df.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:28:41.255505Z","iopub.execute_input":"2024-05-29T06:28:41.255785Z","iopub.status.idle":"2024-05-29T06:28:41.671847Z","shell.execute_reply.started":"2024-05-29T06:28:41.255760Z","shell.execute_reply":"2024-05-29T06:28:41.670862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling and Hyper Parameter Tuning\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport keras_tuner as kt\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef r2_score_metric(y_true, y_pred):\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return 1 - SS_res/(SS_tot + K.epsilon())\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(cleaned_train_df, cleaned_target_df, test_size=0.3, random_state=11)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef build_model(hp):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)))\n\n    for i in range(hp.Int('num_layers', 1, 3)):\n        model.add(tf.keras.layers.Dense(\n            units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n            activation='relu',\n            kernel_regularizer=tf.keras.regularizers.l1_l2(\n                l1=hp.Float('l1_' + str(i), 1e-5, 1e-2, sampling='LOG'),\n                l2=hp.Float('l2_' + str(i), 1e-5, 1e-2, sampling='LOG')\n            )\n        ))\n        model.add(tf.keras.layers.Dropout(hp.Float('dropout_' + str(i), 0.0, 0.5, step=0.1)))\n\n    model.add(tf.keras.layers.Dense(1))\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')\n        ),\n        loss='mean_squared_error',\n        metrics=[r2_score_metric]\n    )\n    return model\n\n# Create a tuner\ntuner = kt.RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=10,\n    executions_per_trial=2,\n    directory='my_dir',\n    project_name='intro_to_kt'\n)\n\n# Search for the best hyperparameters\ntuner.search(X_train_scaled, y_train, epochs=11, validation_split=0.2)\n\n# Get the best model\nbest_model = tuner.get_best_models(num_models=1)[0]\n\n# Evaluate the best model on the test set\ny_pred = best_model.predict(X_test_scaled)\nr2 = r2_score(y_test, y_pred)\nprint(\"R2 Score of the best model:\", r2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:29:11.065632Z","iopub.execute_input":"2024-05-29T06:29:11.066003Z","iopub.status.idle":"2024-05-29T08:38:42.156131Z","shell.execute_reply.started":"2024-05-29T06:29:11.065974Z","shell.execute_reply":"2024-05-29T08:38:42.155116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T08:42:49.477205Z","iopub.execute_input":"2024-05-29T08:42:49.477897Z","iopub.status.idle":"2024-05-29T08:42:49.498196Z","shell.execute_reply.started":"2024-05-29T08:42:49.477866Z","shell.execute_reply":"2024-05-29T08:42:49.497302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nfor key, value in best_hps.values.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T08:42:57.549739Z","iopub.execute_input":"2024-05-29T08:42:57.550752Z","iopub.status.idle":"2024-05-29T08:42:57.556024Z","shell.execute_reply.started":"2024-05-29T08:42:57.550717Z","shell.execute_reply":"2024-05-29T08:42:57.555076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_fixed_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)))\n    model.add(tf.keras.layers.Dense(242, activation='relu'))\n    model.add(tf.keras.layers.Dense(12, activation='relu'))\n    model.add(tf.keras.layers.Dense(1))\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='mean_squared_error',\n        metrics=[r2_score_metric]\n    )\n    return model\n\n# Build and compile the model\nfixed_model = build_fixed_model()\n\n# Print the summary of the model\nfixed_model.summary()\n\n# Train the model\nfixed_model.fit(X_train_scaled, y_train, epochs=5, validation_split=0.2)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T08:43:03.889730Z","iopub.execute_input":"2024-05-29T08:43:03.890548Z","iopub.status.idle":"2024-05-29T08:45:58.591292Z","shell.execute_reply.started":"2024-05-29T08:43:03.890515Z","shell.execute_reply":"2024-05-29T08:45:58.590426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the Model\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n# Evaluate the model on the test set\ny_pred = fixed_model.predict(X_test_scaled)\n\n# Calculate and print detailed metrics\nr2 = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\n\nprint(\"RÂ² Score of the fixed model:\", r2)\nprint(\"Mean Squared Error of the fixed model:\", mse)\nprint(\"Mean Absolute Error of the fixed model:\", mae)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T08:55:11.864545Z","iopub.execute_input":"2024-05-29T08:55:11.865461Z","iopub.status.idle":"2024-05-29T08:55:26.064566Z","shell.execute_reply.started":"2024-05-29T08:55:11.865420Z","shell.execute_reply":"2024-05-29T08:55:26.063565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\ntest1_df = test_df.drop('id',axis=1)\n\n# 3. Make Predictions\nX_test = scaler.transform(test1_df)  # Preprocess the test data\npredictions = fixed_model.predict(X_test)  # Make predictions\n\n# 4. Prepare Submission File\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],  # Assuming 'id' is the identifier column\n    'FloodProbability': predictions.flatten()  # Assuming 'FloodProbability' is the target column\n})\n\n# Save the submission DataFrame to a CSV file\nsubmission_df.to_csv('submission.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T08:55:34.602377Z","iopub.execute_input":"2024-05-29T08:55:34.602826Z","iopub.status.idle":"2024-05-29T08:56:17.874872Z","shell.execute_reply.started":"2024-05-29T08:55:34.602793Z","shell.execute_reply":"2024-05-29T08:56:17.874095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}